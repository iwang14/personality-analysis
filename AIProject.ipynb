{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "4bae0ac5",
      "metadata": {
        "id": "4bae0ac5"
      },
      "source": [
        "<h1>16 Personalities </h1>\n",
        "\n",
        "<h2>1) Understand the classification task for your dataset </h2>\n",
        "<p> The dataset chosen is a synthetic dataset created for practicing classification on for the results of the Myers-Briggs 16 Personality Test. The test has 60 questions. Each question can be answered by choosing a circle from a 7-point scale ranging from Agree to Disagree. After answering all the questions, the test will assign you one of the 16 different 4-letter Myers-Briggs personality types, based on your answers. Since each question tests for certain traits pertaining to different personality types, the answer to each question of the test can be considered as the features for determining what result a person taking the test will get. This dataset will require a multi-class classification, as each of the 16 personality types will be a class. The goal of this classification is to accurately identify which personality type the sample scored given their answers to the questions of the test.</p>\n",
        "\n",
        "<h3>Brief Explanation of the Myers-Briggs typing system</h3>\n",
        "<p>Each Myers-Briggs personality type(MBTI) consists of 4 letters, each representing one of a 2-letter dichotomy. The first letter, E/I represents Extroversion/Introversion. The second letter, N/S, represents Intuition/Sensing. The third letter, F/T, represents Feeling/Thinking. And the final letter, J/P, represents Judging/Perceiving. For example, the ENFJ would be interpreted as Extroversion, Intuition, Feeling, Judging.</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "230eec42",
      "metadata": {
        "id": "230eec42"
      },
      "source": [
        "<h2>2) Analyze your dataset</h2>\n",
        "<p> This dataset contains 59999 samples. Each of the 60 questions is a feature. The dataset also contains 2 additional features, a Response Id for identifying each sample and the Personality, which is the test result of each sample and as such the target of the classification. </p>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79554e6f",
      "metadata": {
        "id": "79554e6f"
      },
      "source": [
        "<h2>3) Feature Engineering</h2>\n",
        "<p>Apart from each question of the test, another feature that may be helpful for the classification would be gender. There are certain personality types that are more prevalent in one gender over the other, for instance, Thinker types (types with XXTX like INTJ, ESTP, etc) are more common in males and may help the classifier in deducing a final type. Among the features provided, evidently the Response Id feature can be discarded as it is only included for identification purposes and is not correlated to the result.</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ccd9322",
      "metadata": {
        "id": "9ccd9322"
      },
      "source": [
        "<h2> 4) Encode the features </h2>\n",
        "<p>All the features take on a range of values from -3 to 3, representing the scale of Disagree to Agree with Disagree being -3 and Agree being 3. Although the features' values are discrete, they contain negative values which may have to be encoded for certain Naive Bayes classifiers, namely the MultinomialNB classifier for discrete features. As such all values were shifted by +3 such that -3 would be 0, -2 would be 1, -1 would be 2, so on and so forth.</p>\n",
        "\n",
        "<p>In addition, since each personality is represented as a 4 character string, the Personality feature was encoded to a numeric value. Each personality is assigned a number from 0-15.</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0dcfdcc8",
      "metadata": {
        "id": "0dcfdcc8"
      },
      "source": [
        "<h2>5-9) Testing the data</h2>\n",
        "<h3> Importing the Dataset </h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24a6a0ec",
      "metadata": {
        "id": "24a6a0ec"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn import linear_model\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "#helper function for concatenating numpy.ndarrays\n",
        "def concatenate(a1,a2, start):\n",
        "    index=start\n",
        "    for x in a2:\n",
        "        a1[index]=x\n",
        "        index=index+1\n",
        "    return a1\n",
        "\n",
        "file=pd.read_csv('16P.csv', header = 0, encoding='cp1252')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e762402f",
      "metadata": {
        "id": "e762402f"
      },
      "source": [
        " <h3>Encoding the features</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0d2fbd3",
      "metadata": {
        "id": "a0d2fbd3"
      },
      "outputs": [],
      "source": [
        "label_personality=LabelEncoder()\n",
        "label_personality.fit([\"ESTJ\", \"ENTJ\", \"ESFJ\", \"ENFJ\", \"ISTJ\", \"ISFJ\", \"INTJ\", \"INFJ\", \"ESTP\", \"ESFP\", \"ENTP\", \"ENFP\", \"ISTP\", \"ISFP\",\"INTP\", \"INFP\"])\n",
        "file[\"Personality\"]=label_personality.fit_transform(file[\"Personality\"])\n",
        "X=(file.drop(\"Personality\", axis=1)).drop(\"Response Id\", axis=1)\n",
        "codes = {-3:0, -2:1, -1:2, 0:3, 1:4, 2:5, 3:6}\n",
        "\n",
        "for features in X:\n",
        "    X[features]=X[features].map(codes)\n",
        "\n",
        "y=file['Personality']\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d876ff3",
      "metadata": {
        "id": "5d876ff3"
      },
      "source": [
        "<h3>Cross Validation</h3>\n",
        "<p> For the cross-validation, we will be using th KFold library to split the dataset. The k-fold size used for this experiment was 5.<p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ea5f71b",
      "metadata": {
        "id": "0ea5f71b"
      },
      "outputs": [],
      "source": [
        "kf = KFold(n_splits=5, random_state=None, shuffle=False) "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9cd29d0a",
      "metadata": {
        "id": "9cd29d0a"
      },
      "source": [
        "<h3> Naive Bayes </h3>\n",
        "<p>For Naive Bayes Classifier, we used the MultinomialNB library because the features values are discrete. </p>\n",
        "<h4>Configuration 1</h4>\n",
        "<p> For the first Naive Bayes classification, the default parameters were used. fit_prior=True, alpha=1.0</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d3e3d35",
      "metadata": {
        "id": "5d3e3d35",
        "outputId": "349426bb-3218-4d19-e797-35d4c2b7552e",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.91      0.89      3743\n",
            "           1       0.87      0.94      0.90      3760\n",
            "           2       0.91      0.94      0.92      3737\n",
            "           3       0.94      0.94      0.94      3760\n",
            "           4       0.81      0.74      0.78      3746\n",
            "           5       0.94      0.92      0.93      3769\n",
            "           6       0.89      0.88      0.89      3759\n",
            "           7       0.91      0.92      0.92      3749\n",
            "           8       0.87      0.88      0.88      3761\n",
            "           9       0.86      0.81      0.83      3734\n",
            "          10       0.82      0.90      0.86      3743\n",
            "          11       0.83      0.90      0.86      3742\n",
            "          12       0.86      0.80      0.83      3739\n",
            "          13       0.91      0.85      0.88      3746\n",
            "          14       0.88      0.81      0.84      3756\n",
            "          15       0.85      0.88      0.87      3755\n",
            "\n",
            "    accuracy                           0.88     59999\n",
            "   macro avg       0.88      0.88      0.88     59999\n",
            "weighted avg       0.88      0.88      0.88     59999\n",
            "\n",
            "[[3416   86    5    6   34    4    4   22    9   23   52   22   35    6\n",
            "     1   18]\n",
            " [  47 3516    7   11   24    9    7    5    6    5   43   44   15    9\n",
            "     7    5]\n",
            " [  20    5 3496    6   16   18    9   13    8   61   24    6    3   24\n",
            "    24    4]\n",
            " [  27   15   64 3520   16    5   18    7    8    6    1    6   12   41\n",
            "     6    8]\n",
            " [ 117   90   22   17 2786   28   58  100   92   30  139  152   19   32\n",
            "    37   27]\n",
            " [  11   41    8    2   24 3472   36   39   19    6    8   26   21    6\n",
            "    11   39]\n",
            " [   6   31   18   14   96   22 3307    9   23   59   13   44   56   22\n",
            "    28   11]\n",
            " [  11   10   13    9  118   40    1 3457   15   19   17   11    5    2\n",
            "    16    5]\n",
            " [   5    5    9   15   60   30   15   24 3313   39   26   53    9   36\n",
            "   112   10]\n",
            " [ 129   16  119    6   36    3   64   71   59 3023   26   11  112   21\n",
            "    29    9]\n",
            " [  22   19    5   12   66   12    3   21   20   15 3382   39   59   16\n",
            "    40   12]\n",
            " [  15   18    4   14   31   17    3   10    6    9  171 3376   12    2\n",
            "    42   12]\n",
            " [  26   58   10   17   13    8  165    7   57  104   59    6 3007   36\n",
            "    25  141]\n",
            " [   7   81   12   62   26    6    6    3  118   56   42   10   18 3193\n",
            "    16   90]\n",
            " [   7   23   14    4   61    2    5    7   17   12   28  281   55    5\n",
            "  3055  180]\n",
            " [  29    8   23   17   17   18    1    5   18   54  103    4   60   44\n",
            "    32 3322]]\n"
          ]
        }
      ],
      "source": [
        "#Training\n",
        "nb1_all_preds=np.ndarray(59999, int)\n",
        "index=0\n",
        "for train_index, test_index in kf.split(X):\n",
        "    \n",
        "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "    MNBclf = MultinomialNB()\n",
        "    model = MNBclf.fit(X_train, y_train)\n",
        "    preds = MNBclf.predict(X_test)\n",
        "    nb1_all_preds=concatenate(nb1_all_preds, preds,index)\n",
        "    index=index+(preds.size)\n",
        "print(classification_report(y, nb1_all_preds))\n",
        "print(confusion_matrix(y, nb1_all_preds))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2a17e5f",
      "metadata": {
        "id": "c2a17e5f"
      },
      "source": [
        "<h3>Configuration 2</h3>\n",
        "<p>For this test, we set the fit_prior=False, alpha=1.0. The fit_prior parameter will use a uniform prior instead of learning the class prior probabilities.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b6f4a1f",
      "metadata": {
        "id": "1b6f4a1f",
        "outputId": "8e84eb8d-bcf4-4168-bde4-17668eea326f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.91      0.89      3743\n",
            "           1       0.88      0.94      0.90      3760\n",
            "           2       0.91      0.94      0.92      3737\n",
            "           3       0.94      0.94      0.94      3760\n",
            "           4       0.81      0.74      0.78      3746\n",
            "           5       0.94      0.92      0.93      3769\n",
            "           6       0.89      0.88      0.89      3759\n",
            "           7       0.91      0.92      0.92      3749\n",
            "           8       0.87      0.88      0.88      3761\n",
            "           9       0.86      0.81      0.83      3734\n",
            "          10       0.82      0.90      0.86      3743\n",
            "          11       0.83      0.90      0.86      3742\n",
            "          12       0.86      0.81      0.83      3739\n",
            "          13       0.91      0.85      0.88      3746\n",
            "          14       0.88      0.81      0.84      3756\n",
            "          15       0.85      0.89      0.87      3755\n",
            "\n",
            "    accuracy                           0.88     59999\n",
            "   macro avg       0.88      0.88      0.88     59999\n",
            "weighted avg       0.88      0.88      0.88     59999\n",
            "\n",
            "[[3418   84    5    6   34    4    4   22    9   22   53   22   35    6\n",
            "     1   18]\n",
            " [  46 3516    8   11   24    9    7    5    6    5   43   44   15    9\n",
            "     7    5]\n",
            " [  20    5 3496    6   16   18    9   13    8   61   24    6    3   24\n",
            "    24    4]\n",
            " [  27   15   66 3518   16    5   18    7    8    6    1    6   12   41\n",
            "     6    8]\n",
            " [ 117   89   23   17 2789   28   58  100   92   30  138  152   20   31\n",
            "    35   27]\n",
            " [  12   40    8    2   24 3470   37   39   19    6    8   26   22    6\n",
            "    11   39]\n",
            " [   6   31   18   14   94   22 3307    9   23   59   13   44   57   22\n",
            "    29   11]\n",
            " [  12   10   13    9  118   40    1 3456   15   19   17   11    5    2\n",
            "    16    5]\n",
            " [   5    5   10   15   60   29   15   24 3313   39   26   54    9   36\n",
            "   112    9]\n",
            " [ 129   16  119    6   35    3   64   71   59 3023   25   11  113   21\n",
            "    29   10]\n",
            " [  22   19    5   12   65   12    3   21   20   14 3382   39   60   16\n",
            "    41   12]\n",
            " [  16   19    4   14   32   17    3   10    6    9  172 3372   12    2\n",
            "    42   12]\n",
            " [  26   57   10   17   13    8  163    7   58  105   57    6 3013   36\n",
            "    24  139]\n",
            " [   7   81   12   62   26    6    6    3  117   56   42   10   18 3195\n",
            "    16   89]\n",
            " [   7   22   14    4   61    2    5    8   18   12   28  280   56    5\n",
            "  3058  176]\n",
            " [  28    7   23   16   17   18    1    5   18   54  103    4   60   44\n",
            "    32 3325]]\n"
          ]
        }
      ],
      "source": [
        "#Training\n",
        "nb2_all_preds=np.ndarray(59999, int)\n",
        "index=0\n",
        "for train_index, test_index in kf.split(X):\n",
        "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "    MNBclf = MultinomialNB(fit_prior=False)\n",
        "    model = MNBclf.fit(X_train, y_train)\n",
        "    preds = MNBclf.predict(X_test)\n",
        "    nb2_all_preds=concatenate(nb2_all_preds, preds,index)\n",
        "    index=index+(preds.size)\n",
        "print(classification_report(y, nb2_all_preds))\n",
        "print(confusion_matrix(y, nb2_all_preds))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20730f04",
      "metadata": {
        "id": "20730f04"
      },
      "source": [
        "<h3>Configuration 3</h3>\n",
        "<p>For this test, we set the fit_prior=True, alpha=100. </p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edab0bcd",
      "metadata": {
        "id": "edab0bcd",
        "outputId": "fe4b83f7-7095-4af2-cbb3-d39dedecd475"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.91      0.89      3743\n",
            "           1       0.87      0.94      0.90      3760\n",
            "           2       0.91      0.94      0.92      3737\n",
            "           3       0.94      0.94      0.94      3760\n",
            "           4       0.82      0.74      0.78      3746\n",
            "           5       0.94      0.92      0.93      3769\n",
            "           6       0.89      0.88      0.89      3759\n",
            "           7       0.91      0.92      0.92      3749\n",
            "           8       0.88      0.88      0.88      3761\n",
            "           9       0.86      0.81      0.83      3734\n",
            "          10       0.82      0.90      0.86      3743\n",
            "          11       0.82      0.90      0.86      3742\n",
            "          12       0.86      0.80      0.83      3739\n",
            "          13       0.91      0.85      0.88      3746\n",
            "          14       0.88      0.81      0.84      3756\n",
            "          15       0.85      0.89      0.87      3755\n",
            "\n",
            "    accuracy                           0.88     59999\n",
            "   macro avg       0.88      0.88      0.88     59999\n",
            "weighted avg       0.88      0.88      0.88     59999\n",
            "\n",
            "[[3416   85    5    6   34    5    4   22    9   23   52   22   35    6\n",
            "     1   18]\n",
            " [  46 3517    7   12   24    9    7    5    6    5   42   44   15    9\n",
            "     7    5]\n",
            " [  20    5 3497    6   15   18    9   13    8   61   24    6    3   24\n",
            "    24    4]\n",
            " [  28   15   64 3520   15    5   18    7    8    6    1    6   12   41\n",
            "     6    8]\n",
            " [ 118   91   23   17 2769   29   61  103   95   31  139  152   20   32\n",
            "    39   27]\n",
            " [  12   40    8    2   23 3475   36   39   17    6    8   26   21    6\n",
            "    11   39]\n",
            " [   6   31   18   14   92   22 3311    9   23   59   13   44   55   22\n",
            "    29   11]\n",
            " [  12   10   13    9  116   40    1 3458   15   19   17   11    5    2\n",
            "    16    5]\n",
            " [   5    5    9   15   60   30   15   24 3312   39   26   54    9   36\n",
            "   112   10]\n",
            " [ 129   16  119    6   34    3   65   71   59 3024   26   11  112   22\n",
            "    28    9]\n",
            " [  22   19    5   12   61   12    3   21   20   15 3387   39   59   16\n",
            "    40   12]\n",
            " [  15   18    4   14   31   18    3   10    6    9  169 3378   11    2\n",
            "    42   12]\n",
            " [  26   58   10   17   13    8  165    7   56  104   60    7 3004   36\n",
            "    26  142]\n",
            " [   6   81   12   63   26    6    6    3  115   56   42   10   17 3198\n",
            "    16   89]\n",
            " [   7   22   14    4   59    2    5    7   17   12   28  283   55    5\n",
            "  3056  180]\n",
            " [  28    7   23   17   16   18    1    5   17   54  102    4   60   44\n",
            "    31 3328]]\n"
          ]
        }
      ],
      "source": [
        "#Training\n",
        "nb3_all_preds=np.ndarray(59999, int)\n",
        "index=0\n",
        "for train_index, test_index in kf.split(X):\n",
        "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "    MNBclf = MultinomialNB(alpha=100)\n",
        "    model = MNBclf.fit(X_train, y_train)\n",
        "    preds = MNBclf.predict(X_test)\n",
        "    nb3_all_preds=concatenate(nb3_all_preds, preds,index)\n",
        "    index=index+(preds.size)\n",
        "print(classification_report(y, nb3_all_preds))\n",
        "print(confusion_matrix(y, nb3_all_preds))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19a21612",
      "metadata": {
        "id": "19a21612"
      },
      "source": [
        "<h2>Logistic Regression</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9f7bc82",
      "metadata": {
        "id": "e9f7bc82"
      },
      "source": [
        "<h3>Configuration 1</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd8317a8",
      "metadata": {
        "id": "cd8317a8"
      },
      "source": [
        "<p>For Configuration 1 we used the default parameters.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2253340",
      "metadata": {
        "id": "b2253340",
        "outputId": "5c9f9238-d213-4374-98e0-7745bf7251f4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\sakee\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "C:\\Users\\sakee\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "C:\\Users\\sakee\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "C:\\Users\\sakee\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.93      0.93      3743\n",
            "           1       0.94      0.94      0.94      3760\n",
            "           2       0.95      0.95      0.95      3737\n",
            "           3       0.95      0.95      0.95      3760\n",
            "           4       0.88      0.87      0.87      3746\n",
            "           5       0.95      0.95      0.95      3769\n",
            "           6       0.93      0.94      0.94      3759\n",
            "           7       0.94      0.94      0.94      3749\n",
            "           8       0.93      0.92      0.92      3761\n",
            "           9       0.90      0.90      0.90      3734\n",
            "          10       0.89      0.91      0.90      3743\n",
            "          11       0.92      0.92      0.92      3742\n",
            "          12       0.90      0.90      0.90      3739\n",
            "          13       0.92      0.91      0.92      3746\n",
            "          14       0.91      0.90      0.90      3756\n",
            "          15       0.91      0.92      0.91      3755\n",
            "\n",
            "    accuracy                           0.92     59999\n",
            "   macro avg       0.92      0.92      0.92     59999\n",
            "weighted avg       0.92      0.92      0.92     59999\n",
            "\n",
            "[[3488   21    7   13   36    5    7   12   16   30   23   19   29    9\n",
            "     7   21]\n",
            " [  23 3547    7   15   30   14    8    4    5    8   18   20   24   18\n",
            "     8   11]\n",
            " [   7    3 3544   10   13   15   19   14    6   35   17    5    7   17\n",
            "    19    6]\n",
            " [  21   17   21 3572   11    4   19   14    8    8    1   11   12   32\n",
            "     4    5]\n",
            " [  46   30    8   20 3260   31   42   42   54   21   54   44   12   21\n",
            "    38   23]\n",
            " [  10   15    7    1   22 3567   31   18    9    6    6   17   28    5\n",
            "     7   20]\n",
            " [   5   19    8    5   26   15 3538    6   15   36    9   15   33   10\n",
            "    11    8]\n",
            " [   6    7   18    4   57   29    5 3530   12   22   28    7    6    3\n",
            "     6    9]\n",
            " [   5    4    7   11   52   17   10   22 3447   12   18   38    7   65\n",
            "    27   19]\n",
            " [  54    7   45   10   41    4   38   26   24 3343   26    8   46   12\n",
            "    35   15]\n",
            " [  24    8    3    9   58   19    6   25   12   22 3399   25   51   22\n",
            "    46   14]\n",
            " [  16   21    4   16   42   19    7    9   15   14   48 3443   12    6\n",
            "    62    8]\n",
            " [  24   23    8   10    4    9   56    8   19   40   62   12 3369   25\n",
            "    20   50]\n",
            " [   7   32    9   30   15    6    8    4   44   45   36    6   17 3418\n",
            "    12   57]\n",
            " [  12   13   18    5   30    7    3    9   26   35   27   74   38    7\n",
            "  3378   74]\n",
            " [  20    2   10   16   14   12    1    4   10   33   53    5   46   43\n",
            "    33 3453]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\sakee\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ],
      "source": [
        "#Training\n",
        "lr1_all_preds=np.ndarray(59999, int)\n",
        "index=0\n",
        "for train_index, test_index in kf.split(X):\n",
        "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "    clf = linear_model.LogisticRegression().fit(X, y)\n",
        "    preds = clf.predict(X_test)\n",
        "    lr1_all_preds=concatenate(lr1_all_preds, preds,index)\n",
        "    index=index+(preds.size)\n",
        "print(classification_report(y, lr1_all_preds))\n",
        "print(confusion_matrix(y, lr1_all_preds))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79e86865",
      "metadata": {
        "id": "79e86865"
      },
      "source": [
        "<h3>Configuration 2</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90217c35",
      "metadata": {
        "id": "90217c35"
      },
      "source": [
        "<p>For Configuration 2 we used the default parameters except for fit_intercept which was changed from True to False.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac174440",
      "metadata": {
        "id": "ac174440",
        "outputId": "3ac13683-249d-4267-f0e9-b7b651d8af61"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\sakee\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "C:\\Users\\sakee\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "C:\\Users\\sakee\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "C:\\Users\\sakee\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.93      0.93      3743\n",
            "           1       0.94      0.94      0.94      3760\n",
            "           2       0.95      0.95      0.95      3737\n",
            "           3       0.95      0.95      0.95      3760\n",
            "           4       0.88      0.87      0.87      3746\n",
            "           5       0.95      0.95      0.95      3769\n",
            "           6       0.93      0.94      0.94      3759\n",
            "           7       0.94      0.94      0.94      3749\n",
            "           8       0.93      0.92      0.92      3761\n",
            "           9       0.90      0.90      0.90      3734\n",
            "          10       0.89      0.91      0.90      3743\n",
            "          11       0.92      0.92      0.92      3742\n",
            "          12       0.90      0.90      0.90      3739\n",
            "          13       0.92      0.91      0.92      3746\n",
            "          14       0.91      0.90      0.91      3756\n",
            "          15       0.91      0.92      0.91      3755\n",
            "\n",
            "    accuracy                           0.92     59999\n",
            "   macro avg       0.92      0.92      0.92     59999\n",
            "weighted avg       0.92      0.92      0.92     59999\n",
            "\n",
            "[[3487   22    7   13   36    5    7   12   15   30   24   19   29    9\n",
            "     7   21]\n",
            " [  23 3546    7   15   29   14    8    4    5    8   18   21   25   18\n",
            "     8   11]\n",
            " [   7    3 3545   10   12   15   19   15    6   34   17    6    7   17\n",
            "    18    6]\n",
            " [  21   17   21 3572   11    4   19   14    8    8    1   11   12   32\n",
            "     4    5]\n",
            " [  47   34    8   19 3260   32   41   38   53   21   55   43   13   21\n",
            "    38   23]\n",
            " [  11   15    7    1   22 3567   31   18    9    6    5   17   28    5\n",
            "     7   20]\n",
            " [   5   19    8    5   26   15 3538    6   16   35    9   15   33   10\n",
            "    11    8]\n",
            " [   6    6   18    4   57   29    5 3532   12   22   27    7    6    3\n",
            "     6    9]\n",
            " [   5    4    7   11   51   17   10   21 3450   12   18   37    7   64\n",
            "    28   19]\n",
            " [  54    7   45   10   42    4   38   26   24 3342   23    8   45   15\n",
            "    35   16]\n",
            " [  25    8    3    9   58   17    6   26   12   23 3393   23   55   23\n",
            "    48   14]\n",
            " [  15   21    4   16   40   19    7   10   15   14   47 3451   13    6\n",
            "    55    9]\n",
            " [  23   20    8   10    4    9   56    9   21   40   60   12 3373   25\n",
            "    21   48]\n",
            " [   7   32    9   30   15    7    7    4   44   45   36    6   17 3418\n",
            "    12   57]\n",
            " [  12   13   18    5   29    7    3    9   26   36   26   75   38    7\n",
            "  3379   73]\n",
            " [  20    2   10   16   14   12    1    4   10   33   53    5   46   43\n",
            "    34 3452]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\sakee\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ],
      "source": [
        "#Training\n",
        "lr2_all_preds=np.ndarray(59999, int)\n",
        "index=0\n",
        "for train_index, test_index in kf.split(X):\n",
        "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "    clf = linear_model.LogisticRegression(fit_intercept = False).fit(X, y)\n",
        "    preds = clf.predict(X_test)\n",
        "    lr2_all_preds=concatenate(lr2_all_preds, preds,index)\n",
        "    index=index+(preds.size)\n",
        "print(classification_report(y, lr2_all_preds))\n",
        "print(confusion_matrix(y, lr2_all_preds))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20a13c25",
      "metadata": {
        "id": "20a13c25"
      },
      "source": [
        "<h3>Configuration 3</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf493019",
      "metadata": {
        "id": "cf493019"
      },
      "source": [
        "<p>For Configuration 3 we used the default parameters except for the solver which was changed from 'lbgfs' to 'liblinear'.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fae44a6",
      "metadata": {
        "id": "3fae44a6",
        "outputId": "f0709903-cf0c-4c10-98e9-6625d03d0bd5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.93      0.92      3743\n",
            "           1       0.93      0.94      0.93      3760\n",
            "           2       0.95      0.95      0.95      3737\n",
            "           3       0.95      0.95      0.95      3760\n",
            "           4       0.87      0.84      0.85      3746\n",
            "           5       0.94      0.95      0.95      3769\n",
            "           6       0.93      0.93      0.93      3759\n",
            "           7       0.93      0.95      0.94      3749\n",
            "           8       0.92      0.91      0.91      3761\n",
            "           9       0.90      0.89      0.89      3734\n",
            "          10       0.88      0.91      0.90      3743\n",
            "          11       0.90      0.92      0.91      3742\n",
            "          12       0.88      0.88      0.88      3739\n",
            "          13       0.92      0.89      0.91      3746\n",
            "          14       0.92      0.89      0.90      3756\n",
            "          15       0.90      0.92      0.91      3755\n",
            "\n",
            "    accuracy                           0.92     59999\n",
            "   macro avg       0.92      0.92      0.92     59999\n",
            "weighted avg       0.92      0.92      0.92     59999\n",
            "\n",
            "[[3473   27   10   12   42    5    5   10   17   33   19   23   32    8\n",
            "     5   22]\n",
            " [  25 3519    9   14   36   14    9    4    6    6   18   26   36   20\n",
            "     9    9]\n",
            " [  11    4 3545   11   14   11   18   15    7   29   17    7    3   21\n",
            "    17    7]\n",
            " [  26    7   26 3576   11    4   14   12    9    8    0   11   15   28\n",
            "     6    7]\n",
            " [  56   49   13   21 3138   36   53   65   63   19   60   72   16   22\n",
            "    32   31]\n",
            " [   7   13    7    1   16 3597   24   17    7    6    5   18   22    5\n",
            "     8   16]\n",
            " [   6   25    8    6   21   15 3507    9   21   35   14   21   41   11\n",
            "    11    8]\n",
            " [   8    8   16    6   50   26    5 3547   10   22   23    4    6    3\n",
            "     4   11]\n",
            " [   6    5    7   15   63   20   10   22 3429   11   16   43    8   46\n",
            "    42   18]\n",
            " [  42    8   47   11   47    4   41   36   23 3307   25   11   63   19\n",
            "    30   20]\n",
            " [  22   12    4   11   52   19    6   19   10   22 3397   22   60   19\n",
            "    53   15]\n",
            " [  16   18    7   18   40   19    8   10   16   14   53 3448   17    5\n",
            "    36   17]\n",
            " [  31   23    6   10    7    9   67    8   29   55   59   16 3305   39\n",
            "    13   62]\n",
            " [   4   47   13   28   28   10    8    6   52   42   56   11   22 3348\n",
            "    12   59]\n",
            " [  16   26   18    4   42    7    3   11   24   36   25   77   40    6\n",
            "  3338   83]\n",
            " [  18    4   12   17   14   14    1    5   12   33   61    4   51   48\n",
            "    24 3437]]\n"
          ]
        }
      ],
      "source": [
        "#Training\n",
        "lr3_all_preds=np.ndarray(59999, int)\n",
        "index=0\n",
        "for train_index, test_index in kf.split(X):\n",
        "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "    clf = linear_model.LogisticRegression(solver='liblinear').fit(X, y)\n",
        "    preds = clf.predict(X_test)\n",
        "    lr3_all_preds=concatenate(lr3_all_preds, preds,index)\n",
        "    index=index+(preds.size)\n",
        "print(classification_report(y, lr3_all_preds))\n",
        "print(confusion_matrix(y, lr3_all_preds))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7d59b0f",
      "metadata": {
        "id": "d7d59b0f"
      },
      "source": [
        "<h2> Multi-Layer Perceptron</h2>\n",
        "<p>For the Multi-Layer Classifier, the three parameters that we tinkered with were the max_iter, hidden_layer_sizes, and activation parameters.</p>\n",
        "<h3>Configuration 1</h3>\n",
        "<p> max_iter=200, hidden_layer_sizes=(1,100), activation=\"relu\"</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0258c60f",
      "metadata": {
        "id": "0258c60f",
        "outputId": "faeda09f-14ff-4b6c-c2f3-a0da3e6eca01"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\sakee\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "C:\\Users\\sakee\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "C:\\Users\\sakee\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "C:\\Users\\sakee\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.97      0.97      3743\n",
            "           1       0.97      0.97      0.97      3760\n",
            "           2       0.97      0.98      0.97      3737\n",
            "           3       0.97      0.97      0.97      3760\n",
            "           4       0.97      0.96      0.97      3746\n",
            "           5       0.98      0.96      0.97      3769\n",
            "           6       0.97      0.96      0.97      3759\n",
            "           7       0.97      0.97      0.97      3749\n",
            "           8       0.97      0.97      0.97      3761\n",
            "           9       0.96      0.96      0.96      3734\n",
            "          10       0.98      0.97      0.97      3743\n",
            "          11       0.96      0.97      0.96      3742\n",
            "          12       0.97      0.98      0.97      3739\n",
            "          13       0.97      0.97      0.97      3746\n",
            "          14       0.97      0.97      0.97      3756\n",
            "          15       0.97      0.97      0.97      3755\n",
            "\n",
            "    accuracy                           0.97     59999\n",
            "   macro avg       0.97      0.97      0.97     59999\n",
            "weighted avg       0.97      0.97      0.97     59999\n",
            "\n",
            "[[3637    8    4    9    5    6    9    4    7   12    3   10    9   10\n",
            "     1    9]\n",
            " [  11 3666    2    4    9    2    8    3    3    5    7   11   10   10\n",
            "     6    3]\n",
            " [   6    3 3649   10    8    4    5    7    5   11    1    8    9    7\n",
            "     1    3]\n",
            " [   5   13   11 3659    6    6    6    5    8    6    1   11    4    7\n",
            "     7    5]\n",
            " [  10   14    7    7 3612    4   11   15    5    7   11   13    4   10\n",
            "    12    4]\n",
            " [   5    4   15    6   11 3632   13    9    7    8    9   14   18    6\n",
            "     4    8]\n",
            " [  10    9   15   15    7   10 3626    7    9   11    4    9   13    5\n",
            "     5    4]\n",
            " [   6    7   10    7    6    9    4 3653    4    7   13    4    9    2\n",
            "     3    5]\n",
            " [   5    3    7    9    3    8   11   11 3653    4    2   22    6    9\n",
            "     4    4]\n",
            " [  14    7   15   10   10    3   12   12    6 3593    5    8    7    9\n",
            "    13   10]\n",
            " [  12    9    3    4   10    3    7   11   11   14 3614   11    6   14\n",
            "     5    9]\n",
            " [  12    8    6    9    6    6    3    9   16    7    7 3620    6    9\n",
            "    12    6]\n",
            " [   7    7    6    5    2    7    9    5    3   10    7    6 3651    4\n",
            "     3    7]\n",
            " [   4    7    2    9    8    3    6    4    6   11    6    8    9 3641\n",
            "    10   12]\n",
            " [   4   11   10    4    6    6    5    1    8   10    5   13   10    6\n",
            "  3641   16]\n",
            " [   9    1    6    5    6   16    7    3    1   12    7    2    8   16\n",
            "    17 3639]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\sakee\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "#Training\n",
        "mlp1_all_preds=np.ndarray(59999, int)\n",
        "index=0\n",
        "for train_index, test_index in kf.split(X):\n",
        "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
        "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "    #sns.countplot(y_train)\n",
        "    clf = MLPClassifier()\n",
        "    model = clf.fit(X_train, y_train)\n",
        "    preds = clf.predict(X_test)\n",
        "    mlp1_all_preds=concatenate(mlp1_all_preds, preds,index)\n",
        "    index=index+(preds.size)\n",
        "print(classification_report(y, mlp1_all_preds))\n",
        "print(confusion_matrix(y, mlp1_all_preds))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c75970b7",
      "metadata": {
        "id": "c75970b7"
      },
      "source": [
        "<h3>Configuration 2</h3>\n",
        "<p> max_iter=600, hidden_layer_sizes=(1,100),  activation=\"relu\"</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "132649a5",
      "metadata": {
        "id": "132649a5",
        "outputId": "0a3dd17c-27aa-4add-c4a6-b3291eb280e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.97      0.97      3743\n",
            "           1       0.98      0.96      0.97      3760\n",
            "           2       0.97      0.97      0.97      3737\n",
            "           3       0.97      0.96      0.97      3760\n",
            "           4       0.96      0.96      0.96      3746\n",
            "           5       0.96      0.96      0.96      3769\n",
            "           6       0.97      0.96      0.97      3759\n",
            "           7       0.97      0.97      0.97      3749\n",
            "           8       0.95      0.97      0.96      3761\n",
            "           9       0.97      0.96      0.97      3734\n",
            "          10       0.95      0.97      0.96      3743\n",
            "          11       0.96      0.95      0.96      3742\n",
            "          12       0.97      0.97      0.97      3739\n",
            "          13       0.96      0.96      0.96      3746\n",
            "          14       0.97      0.97      0.97      3756\n",
            "          15       0.97      0.97      0.97      3755\n",
            "\n",
            "    accuracy                           0.97     59999\n",
            "   macro avg       0.97      0.97      0.97     59999\n",
            "weighted avg       0.97      0.97      0.97     59999\n",
            "\n",
            "[[3617   10    8    8    8   10    5    7   12   12   12    9    5    6\n",
            "     4   10]\n",
            " [   9 3615    3   11   20    6   14    3    4    5   10   12   17   17\n",
            "     8    6]\n",
            " [   5    2 3642    8    9   10    3   15    8    8    4    4    5    6\n",
            "     3    5]\n",
            " [  10   10   15 3625    7    8    5   11   13    7    2   12    4   17\n",
            "     4   10]\n",
            " [  11   11    5    8 3612    8    5   11    8    7   18   11    6    9\n",
            "     9    7]\n",
            " [   6    6   14    7   10 3613    7   15   16    9   16    8   17   11\n",
            "     5    9]\n",
            " [   8   10    8   13    9   19 3618    6   18   11    3    3   14    9\n",
            "     6    4]\n",
            " [   5    2    9    3   10   14    4 3638    9   12   19    7    5    3\n",
            "     5    4]\n",
            " [   7    2    6    5    6   14   12    8 3637    5   12   18    4   10\n",
            "     8    7]\n",
            " [  10    1   15    5   11    7   11    8   12 3600    8    7   11   11\n",
            "    12    5]\n",
            " [  12    3    5    2   10    5    2    7    8    6 3644   11    4    9\n",
            "     7    8]\n",
            " [   8    5    6   15   13   10    8    9   27    7   21 3572    7   16\n",
            "    12    6]\n",
            " [  10    7    8   10    5    8    9    6   10    5   15   11 3609   10\n",
            "     8    8]\n",
            " [   8   12   11   10   10    4    6    4   12    6   11    5    9 3608\n",
            "    10   20]\n",
            " [   3    8    8    4    7    4    4    7   14    9   14   15    8   10\n",
            "  3626   15]\n",
            " [   9    3    5    9   12   10    4    8    5    9   13    6    9   15\n",
            "     9 3629]]\n"
          ]
        }
      ],
      "source": [
        "#Training\n",
        "mlp2_all_preds=np.ndarray(59999, int)\n",
        "index=0\n",
        "for train_index, test_index in kf.split(X):\n",
        "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
        "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "    #sns.countplot(y_train)\n",
        "    clf = MLPClassifier(max_iter=600)\n",
        "    model = clf.fit(X_train, y_train)\n",
        "    preds = clf.predict(X_test)\n",
        "    mlp2_all_preds=concatenate(mlp2_all_preds, preds,index)\n",
        "    index=index+(preds.size)\n",
        "print(classification_report(y, mlp2_all_preds))\n",
        "print(confusion_matrix(y, mlp2_all_preds))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00198d7a",
      "metadata": {
        "id": "00198d7a"
      },
      "source": [
        "<h3>Configuration 3</h3>\n",
        "<p> max_iter=600, hidden_layer_sizes=(75,25), activation=logistic</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c47c350",
      "metadata": {
        "id": "5c47c350",
        "outputId": "b174d9fd-a644-482e-a22a-f397eccfd3b7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\sakee\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (600) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "C:\\Users\\sakee\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (600) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.96      0.95      3743\n",
            "           1       0.96      0.95      0.96      3760\n",
            "           2       0.96      0.96      0.96      3737\n",
            "           3       0.96      0.96      0.96      3760\n",
            "           4       0.95      0.96      0.96      3746\n",
            "           5       0.95      0.96      0.95      3769\n",
            "           6       0.95      0.95      0.95      3759\n",
            "           7       0.96      0.96      0.96      3749\n",
            "           8       0.95      0.96      0.96      3761\n",
            "           9       0.95      0.95      0.95      3734\n",
            "          10       0.94      0.95      0.95      3743\n",
            "          11       0.94      0.94      0.94      3742\n",
            "          12       0.95      0.95      0.95      3739\n",
            "          13       0.95      0.95      0.95      3746\n",
            "          14       0.96      0.94      0.95      3756\n",
            "          15       0.95      0.95      0.95      3755\n",
            "\n",
            "    accuracy                           0.95     59999\n",
            "   macro avg       0.95      0.95      0.95     59999\n",
            "weighted avg       0.95      0.95      0.95     59999\n",
            "\n",
            "[[3578   11   11    9   10   10    8    6    6   18   19   21   12    8\n",
            "     4   12]\n",
            " [  15 3590    6   10    8   16   17   12    3    8   12   20   15   15\n",
            "     8    5]\n",
            " [   6    5 3593   12   13   13    7   19    6   18    5    7    7    6\n",
            "    10   10]\n",
            " [  17   10   18 3594   12    9   18    5   12    8    8   10    5   24\n",
            "     4    6]\n",
            " [  16   14   12    7 3586   12    7   15    7    8   18   17    1   10\n",
            "     8    8]\n",
            " [  10   12    7    1    7 3602   16   11   11    7   11   16   19   15\n",
            "     7   17]\n",
            " [  10   11    9   18   10   25 3580    5   21   21    5   12   12   11\n",
            "     4    5]\n",
            " [  10   10   22    4   17   15    8 3587   13   19   14    7    7    5\n",
            "     4    7]\n",
            " [   9    4   12   12    8   12   14    7 3601   10   22   16   11    9\n",
            "    10    4]\n",
            " [  21    6   15   13    9    6   17   14   11 3535   21   14   21   14\n",
            "    10    7]\n",
            " [  20    7    6    4   18   13    7   18    9   15 3558   17   11   18\n",
            "    15    7]\n",
            " [  19   11    7   11   27   13   13    7   26   13   21 3517    9   10\n",
            "    26   12]\n",
            " [  15   20    9    8    3   14   24    7    8   10   10   14 3554   12\n",
            "    10   21]\n",
            " [   9   13    6   22   11    6   14    2   11   13   12   10   17 3570\n",
            "     7   23]\n",
            " [   1   11    9    8   11    5    6   14   26   10   20   26   13   22\n",
            "  3543   31]\n",
            " [  14    6    6   12   13   12    7    8    7   17   16    5   13   23\n",
            "    33 3563]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\sakee\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (600) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "#Training\n",
        "mlp3_all_preds=np.ndarray(59999, int)\n",
        "index=0\n",
        "for train_index, test_index in kf.split(X):\n",
        "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
        "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "    #sns.countplot(y_train)\n",
        "    clf = MLPClassifier(max_iter=600, hidden_layer_sizes=(75,25), activation=\"logistic\")\n",
        "    model = clf.fit(X_train, y_train)\n",
        "    preds = clf.predict(X_test)\n",
        "    mlp3_all_preds=concatenate(mlp3_all_preds, preds,index)\n",
        "    index=index+(preds.size)\n",
        "print(classification_report(y, mlp3_all_preds))\n",
        "print(confusion_matrix(y, mlp3_all_preds))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57416181",
      "metadata": {
        "id": "57416181"
      },
      "source": [
        "<h2>10) Analysis</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f51cea9",
      "metadata": {
        "id": "5f51cea9"
      },
      "source": [
        "<p>All classes across all models scored fairly high precision scores. This could be due to the fact that the sample was fairly balanced, with a nearly equal number of each class present in the dataset as shown in the graph below. If we compare the results of each model, the model with the highest precision is the Multi-Layer Perceptron. All classes scored a precision of at least 0.96, indicating an accurate discriminator.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af9df4f8",
      "metadata": {
        "id": "af9df4f8",
        "outputId": "de865dd6-3de9-45a6-e170-2a299a59fcaa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\sakee\\anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<AxesSubplot:xlabel='Personality', ylabel='count'>"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "sns.countplot(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4fc2e23",
      "metadata": {
        "id": "e4fc2e23"
      },
      "source": [
        "<h3>Naive Bayes</h3>\n",
        "For the Naive Bayes model, all classification tests yielded fairly accurate results. Despite the modification of parameters among the three trials, the results were all similar. The second configuration, in which the fit_prior was set to False and a uniform prior was used, resulted in the most accurate test values. \n",
        "<h3>Logistic Regression</h3>\n",
        "<p>After having done the calculations, it was deduced that the default parameters of the logistic regression in configuration 1 and the liblinear parameter in configuration 3 performed the best. Having the highest overall precision and recall scores.\n",
        "\n",
        "For the second configuration, we used the parameter fit_intercept and set it to false (whose default value is true). The parameter fit_intercept refers to the constant number being added to the logistic regression function. \n",
        "\n",
        "Since the sigmoid function is used to predict the probability of a binary variable, the accuracy of our features could be compromised since our features are non-binary.</p>\n",
        "<h3> MLP</h3>\n",
        "<p> For this model, the parameters tinkered with were the max_iter, hidden_layer_sizes, and activation parameters. The default configuration yielded the most accurate results with precision scores in the high 90s(0.90+). This configuration contained 1 hidden layer of 100 neural nodes and the max_iter was set to 200, meaning that the solver would iterate 200 times to train the weights or until convergence. However, with this configuration, 200 iterations was not enough to reach convergence, so the model could still be improved immensely with more iterations. \n",
        "    In configuration 2, the max_iter was increased to 600 to attempt reaching convergence, with all other default parameters remaining the same. Although convergence was reached, the results yielded were marginally less accurate than those of configuration 1. This could be because the number of iterations was too high to the point of the error beginning to increase again. However, configuration 2 still performed better than configuration 3. \n",
        "    Configuration 3 had max_iter=600, hidden_layer_sizes=(75,25) which means 75 hidden layers with 25 nodes, and and a logistic activation function. Although there were 600 iterations, the model did not reach convergence. This could be attributed to the increase in hidden layers which may be disproportionate to the complexity of the problem, causing the data to become overfitted. This would mean that the model can classify the training samples with high accuracy, but fails with new data because it is too closely fitted with the training data. Moreover, the logistic activation function is better suited for binary features, which none of the features in this dataset are.</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "429f0c25",
      "metadata": {
        "id": "429f0c25"
      },
      "source": [
        "<h3> False Positives and False Negatives</h3>\n",
        "To analyze the occurences of false positives and negatives, we will observe the confusion matrix of Naive Bayes - Configuration 1:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15434cbe",
      "metadata": {
        "id": "15434cbe",
        "outputId": "be270229-f6d2-443f-c077-aa0e7e233a28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[3416   86    5    6   34    4    4   22    9   23   52   22   35    6\n",
            "     1   18]\n",
            " [  47 3516    7   11   24    9    7    5    6    5   43   44   15    9\n",
            "     7    5]\n",
            " [  20    5 3496    6   16   18    9   13    8   61   24    6    3   24\n",
            "    24    4]\n",
            " [  27   15   64 3520   16    5   18    7    8    6    1    6   12   41\n",
            "     6    8]\n",
            " [ 117   90   22   17 2786   28   58  100   92   30  139  152   19   32\n",
            "    37   27]\n",
            " [  11   41    8    2   24 3472   36   39   19    6    8   26   21    6\n",
            "    11   39]\n",
            " [   6   31   18   14   96   22 3307    9   23   59   13   44   56   22\n",
            "    28   11]\n",
            " [  11   10   13    9  118   40    1 3457   15   19   17   11    5    2\n",
            "    16    5]\n",
            " [   5    5    9   15   60   30   15   24 3313   39   26   53    9   36\n",
            "   112   10]\n",
            " [ 129   16  119    6   36    3   64   71   59 3023   26   11  112   21\n",
            "    29    9]\n",
            " [  22   19    5   12   66   12    3   21   20   15 3382   39   59   16\n",
            "    40   12]\n",
            " [  15   18    4   14   31   17    3   10    6    9  171 3376   12    2\n",
            "    42   12]\n",
            " [  26   58   10   17   13    8  165    7   57  104   59    6 3007   36\n",
            "    25  141]\n",
            " [   7   81   12   62   26    6    6    3  118   56   42   10   18 3193\n",
            "    16   90]\n",
            " [   7   23   14    4   61    2    5    7   17   12   28  281   55    5\n",
            "  3055  180]\n",
            " [  29    8   23   17   17   18    1    5   18   54  103    4   60   44\n",
            "    32 3322]]\n"
          ]
        }
      ],
      "source": [
        "print(confusion_matrix(y, nb1_all_preds))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f3dd955",
      "metadata": {
        "id": "0f3dd955"
      },
      "source": [
        "We will analyze the \"0\" personality type, the ENFJ. In total, there were 478 False Positives. Personality type 3(ENTP) and 8(INFJ) recorded the highest number of False Positives, with their amounts in the 100s. This could be because the personality types are similar and therefore answer certain questions the same way. In terms of False Negatives, the most notable example for Personality type 0 was personality type 1(ENFP), accounting for 86 of the False Negatives. Similar to the False Positives, this could be attributed to the similarities between the two types in their behaviour and in turn their possible answers to the test questions. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19ddf14f",
      "metadata": {
        "id": "19ddf14f"
      },
      "source": [
        "<h2>References</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c54d61fa",
      "metadata": {
        "id": "c54d61fa"
      },
      "source": [
        "Dataset: https://www.kaggle.com/datasets/anshulmehtakaggl/60k-responses-of-16-personalities-test-mbt\n",
        "https://www.analyticsvidhya.com/blog/2018/05/improve-model-performance-cross-validation-in-python-r/\n",
        "https://www.tutorialspoint.com/scikit_learn/scikit_learn_classification_with_naive_bayes.htm\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ff101e4",
      "metadata": {
        "id": "2ff101e4"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
